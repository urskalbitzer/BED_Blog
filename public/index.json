[{"authors":["admin"],"categories":null,"content":"For my research, I analyze behavioural, ecological, geographic, socio-economic and other types of data. Usually, the first steps are to figure out how and from where to get the data (if they are publicly available), to do some first cleaning and pre-processing, and then to test potential methods to obtain insights from the data.\nSince I very often benefit from other blogs while working with data, and because I am absolutely fascinated about all the possibilities that arise from public data and open source tools, my aim for this blog is to share some of my data projects. My hope is that other people benefit from this blog just as I benefit from all the great resources made available by other people.\nFor more information about my research, please look at my webpage.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://bedatablog.netlify.com/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"For my research, I analyze behavioural, ecological, geographic, socio-economic and other types of data. Usually, the first steps are to figure out how and from where to get the data (if they are publicly available), to do some first cleaning and pre-processing, and then to test potential methods to obtain insights from the data.\nSince I very often benefit from other blogs while working with data, and because I am absolutely fascinated about all the possibilities that arise from public data and open source tools, my aim for this blog is to share some of my data projects.","tags":null,"title":"Urs Kalbitzer","type":"authors"},{"authors":["Patrick Lauer"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"58244620c321737d31f6c63aa0eca9c4","permalink":"https://bedatablog.netlify.com/authors/patrick-lauer/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/patrick-lauer/","section":"authors","summary":"","tags":null,"title":"Patrick Lauer","type":"authors"},{"authors":[],"categories":[],"content":" Conception, Idea, and Writing: Patrick Lauer and Urs Kalbitzer. Realization and Programming: Patrick Lauer.\nIntroduction Animals forage to meet their nutritional needs while avoiding potentially harmful components, such as toxins. Geometric Framework Models, which have gained popularity in recent years, provide a framework that takes into account that there is rarely one single resource that satisfies an animal’s nutritional requirements and animals have to balance the consumption of different food resources with different nutritional compositions (Lambert \u0026amp; Rothman, 2015; Simpson \u0026amp; Raubenheimer, 2011). Thus, following Geometric Framework Models, the selection of food resources is not just based on general preferences for specific food items and their availability in the environment. Rather it depends on the animal’s nutritional goals, the composition (i.e. nutrients, harmful components) as well as the availability of food resources, and the animal’s current nutritional state (i.e. which nutrients have recently been consumed), see Figure 1.\nEmpirical studies on animal foraging behavior often focus on estimating preferences for different food items, which is usually done by calculating electivity indices (Manly et al., 2007). Such indices have been developed with the assumption that animals consume different food items proportionally to their preferences for, and the availability of, the different items in the environment. This, however, hardly captures the complex decision-making processes involved in foraging.\nAssuming that Geometric Framework Models better reflect animal foraging decisions than just preference models, the applicability of such electivity indices is unclear. Therefore, we here simulate animals foraging according to such Geometric Framework Models to explore nutrient acquisition in different scenarios (Part 1), and then calculate electivity indices based on the observed consumption of different food items to compare these indices with what we know about the foraging behavior of our simulated animals (Part 2).\nFigure 1: Diagram illustrating the assumptions simulated for Geometric Framework Models. The red coloured boxes highlight set parameters in the simulation. General concept of the simulation The basic idea is that the (simulated) animals have an optimal, or target, ratio of different nutrients, for example a ratio of proteins (P) to carbohydrates (C) of 1P:1C. There are different food items in the environment, and these items vary with regard to their nutritional content, for example item A has 3 units protein and 1 unit carbohydrates (= 3P:1C), whereas another item B has 1 unit proteins and 3 units carbohydrates (= 1P:3C). Thus, neither item will allow an animal to stay close to their target ratio of 1P:1C and they have to feed from both items. In other words, after each feeding event, the ratio of consumed nutrients will deviate from the target ratio in one or another direction. This affects which food item should be eaten next. However, in our simulation (and in nature), different items can vary with regard to their availability, it is thereby ‘easier’ for animals to consume more abundant items. This means that which item is selected in each feeding event depends on (i) how much the ratio of consumed nutrients would deviate from the target ratio after eating that item and (ii) the availability of different food items.\nThis simulation is done using the function geometric_framework(), for which the code is provided below. For each individual, the simulation runs through n feeding events. For each feeding event, the function calculates the cumulative units of each nutrient that would be consumed if a particular food item was eaten, and how much the new nutrient ratio would deviate from the target ratio. Based on this deviation and the availability of the item, the function calculates the relative probability of consuming each item, which is then used to determine which item is actually consumed (more details are provided below). The cumulative nutrient units consumed are updated for that individual after each feeding event.\nThis simulation is done for a given number of individuals, which currently all have the same nutritional target. Furthermore, all individuals can feed on the same food items, each item with a defined content of different nutrients. However, we simulate inter-individual variability in the availability of food items (subjective food availability) to mimic natural variation between individuals due to differences in feeding position, cognitive capacities (i.e., skills to find and utilize specific items) and other factors. This variability is realized using a beta-distribution. The function can be used to simulate different kinds of scenarios to explore how animals would move through their nutritional landscapes.\nCurrently, some options are still missing but we will implement them at a later stage:\nAssigning potentially harmful components that an animal wants to avoid.\nSimulating the expenditure of resources while foraging.\nDefining more than two different nutrients per item.\nAdding a “consumption effect”, thus a decrease in availability if an item was consumed.\nMost of these options should be straightforward to implement the way we set up the function.\nCalculating the deviation from optimal nutrient ratio and probability of selecting food items As described above, for each potentially consumed food item, the deviation from the optimal ratio of consumed nutrients has to be calculated. This however, comes with one challenge that is illustrated in Figure 2. Consider a simple example with only two nutrients, a and b, where an animal has an optimal ratio of 1a:1b for these nutrients. If the animal has the option between an item with 3a:1b and another item with 1a:3b, the deviations resulting from consuming each item should be symmetrical, with 3 times more of nutrient a in one case and 3 times more of nutrient b in the other. However, differences between the slope of the original/optimal line (s0 = 1) and the slopes of the two new lines (s1 = 3 for Item 1 and s2 = 1/3 for Item 2) would be 3 - 1 = 2 and 1/3 - 1 = -2/3, respectively.\nFigure 2: Potential deviation from optimal nutrient ratio when consuming different food items. To overcome the challenge explained above, we can use the magnitudes of differences between the optimal slope \\(s\\\\{optimal} =\\frac{optimal{Nutrient1}}{optimal{Nutrient2}}\\) and the potential new slope that arises when consuming a particular item \\(s\\\\{new} =\\frac{potential{Nutrient1}}{potential{Nutrient2}}\\) . This can be done by using the log of the ratio between the new slope and the original slopes \\(log(\\frac{s\\\\{new}}{s\\\\{optimal}})\\) . Using this approach, the deviations become symmetrical and can be compared, as illustrated in Figure 3.\nsnew \u0026lt;- c(1/6:1, 2:6) s_opt_1 \u0026lt;- 1 s_opt_2 \u0026lt;- 0.5 plot(x = log(snew), y = log(snew/s_opt_1), main = \u0026quot;For optimal ratio of 1a:1b\u0026quot;, xaxt = \u0026quot;n\u0026quot;, xlab = \u0026quot;New ratio\u0026quot;, ylab = \u0026quot;Deviation: log(s_new/s_optimal)\u0026quot;) axis(side = 1, at = log(snew), labels = round(snew, 2), las = 2) # Line for deviations lines(x = log(c(0.5, 1, 1, 1, 2)), y = log(c(0.5, 0.5, 1, 2, 2)/s_opt_1), lty = 1, col = \u0026quot;red\u0026quot;) # Optimal line lines(x = c(log(1), log(1)), y = c(log(min(snew)/1), log(max(snew)/1)), lty = 2, col = \u0026quot;grey20\u0026quot;) Figure 3: Potential deviation from optimal nutrient ratio when consuming different food items illustrated as magnitude of slope differences. The x-axis indicates the new ratio after consuming a food item with the vertical dashed line showing the optimal ratio. The y-axis illustrates the deviation calculated as described above. By using this approach the absolute value of the deviation between 1 (the optimal ratio) and the new ratio 1:2 (0.5) is the same as between the optimal ratio of 1 and the new ratio of 2:1 (2).\nTo put this into a formula, the deviation from the optimal ratio after consuming item k by individual j in feeding event i is:\n\\[ deviation_{i, j, k} = log(\\frac{slope_{optimal}}{slope_{i, j, k}}) \\]\nHere, \\(slope_{i,j, k}\\) is the slope of the line between 0 and consumed units of nutrients after the feeding event i.\nIt is important to note that the change in this deviation after each new food item becomes less severe if an individual has already consumed many items. That’s because we are using the cumulative amount of consumed nutrients which decreases the relative importance of each new item for the ratio of already consumed nutrients. In other words, an individual becomes less picky if it has already consumed many nutrients because each new item would only have little impact on the ratio of consumed nutrients.\nWe then define the probability of choosing item k by individual j in feeding event i (\\(P_{i,j,k}\\)) as:\n\\[ P_{i, j, k} = (1 - (\\frac{deviation_{i, j, k}}{threshold})^2) * availabilty_{i,k} \\]\nHere, \\(threshold\\) is the maximum tolerable deviation of the nutritional ratio compared to the optimal ratio before an individual stops feeding on a particular item. And \\(availability_{i,k}\\) is the availability of food item i for individual k. Note that these are relative probabilities (or weights) calculated for each food item (per individual and feeding event) and they don’t sum up to 1 for each potential feeding event. In other words, if the deviation from the optimal value increases, the probability of selecting this specific food item decreases. The multiplication term for food availability \\((1 - (\\frac{deviation_{i, j, k}}{threshold})^2)\\) is illustrated in Figure 4 with different threshold values.\npar(mfrow = c(1,2)) curve((1 - (x/0.5)^2), from = -0.5, to = 0.5, main = \u0026quot;Threshold = 0.5\u0026quot;, xlab = \u0026quot;Deviation from optimal nutrient value\u0026quot;, ylab = \u0026quot;Multiplicator for food availability\u0026quot;) curve((1 - (x/1)^2), from = -1, to = 1, main = \u0026quot;Threshold = 1\u0026quot;, xlab = \u0026quot;Deviation from optimal nutrient value\u0026quot;, ylab = \u0026quot;Multiplicator for food availability\u0026quot;) Figure 4: Weight of multiplication term for food availability with different threshold values. Define the function The function below to simulate animals following Geometric Framework Models follows the logic explained above.\ngeometric_framework \u0026lt;- function(items_nutrients = list(c(3,1), c(1,3), c(1,1)), items_availability = c(0.8, 0.8, 0.8), optimal_ratio = c(2,1), threshold_deviation = 1, n_feeding_events = 100, n_individuals = 10, initial_nutrients = c(0,0), return_class = c(\u0026quot;list\u0026quot;, \u0026quot;df\u0026quot;)) { # If no return class is specified, use first type in vector (here a list) return_class \u0026lt;- match.arg(return_class) ## Do some checks if (length(items_nutrients) != length(items_availability)) stop(\u0026quot;Length of items_nutrients and items_availability differ. Provide nutrients and availability for all items included.\u0026quot;) if(any(lapply(items_nutrients, length) != 2)) stop(\u0026quot;Currently, only two types of nutrients per item allowed.\u0026quot;) if(any(lapply(items_availability, length) != 1)) stop(\u0026quot;Provide one availability value per item.\u0026quot;) ## Define Beta density function parameterized with \u0026quot;prob\u0026quot; and \u0026quot;theta\u0026quot; for adding # \u0026quot;noise\u0026quot; to food availability. Taken from # https://rdrr.io/github/rmcelreath/rethinking/src/R/distributions.r rbeta2 \u0026lt;- function( n , prob , theta ) { a \u0026lt;- prob * theta b \u0026lt;- (1-prob) * theta rbeta( n , shape1 = a , shape2 = b) } ## Initialize Variables and Objects # Prior to the first feeding event, animals current nutrients are the set to initial_nutrients current_nutrients_per_individual \u0026lt;- rep(list(initial_nutrients), n_individuals) # Set up helper variables and vector for probabilities to choose each item n_nutrients \u0026lt;- length(items_nutrients[[1]]) n_items \u0026lt;- length(items_nutrients) probability_weight_choosing_item \u0026lt;- rep(NA, n_items) # Set up matrix for items consumed with feeding events as rows and individuals as columns items_consumed \u0026lt;- matrix(NA, nrow = n_feeding_events, ncol = n_individuals) # Set up array for consumed nutrients with feeding events as rows (first # index), individuals as columns (second index), and the different nutrients in # the third dimension (third index) nutrients_consumed \u0026lt;- array(NA, dim = c(n_feeding_events, n_individuals, n_nutrients)) dimnames(nutrients_consumed) \u0026lt;- list( 1:n_feeding_events, 1:n_individuals, 1:n_nutrients) # Save parameters as attributes attr(nutrients_consumed, \u0026quot;items_nutrients\u0026quot;) \u0026lt;- items_nutrients attr(nutrients_consumed, \u0026quot;items_availability\u0026quot;) \u0026lt;- items_availability attr(nutrients_consumed, \u0026quot;optimal_ratio\u0026quot;) \u0026lt;- optimal_ratio # For each item, add inter-individual variability in food availability using the above defined function rbeta2 availability_per_individual \u0026lt;- t(sapply(items_availability, function(x) rbeta2(n = n_individuals, prob = x, theta = 500))) ## Apply Geometric Framework assumptions to select which food item to include in diet # Go though all feeding events... for (event_i in 1:n_feeding_events) { #... and for each feeding event, through all individuals... for (individual_j in 1:n_individuals) { # ... and for each individual through all items. for (item_k in 1:n_items) { # First, determine how nutrients would look like if item_k is added to current nutrients potential_nutrients \u0026lt;- current_nutrients_per_individual[[individual_j]] + items_nutrients[[item_k]] # Transform optimal ratio and potential ratio from vector to numeric value optimal_ratio_numeric \u0026lt;- optimal_ratio[1] / optimal_ratio[2] potential_nutrient_ratio_numeric \u0026lt;- potential_nutrients[1] / potential_nutrients[2] # Calculate the deviation from the optimal nutrient acquisition if item_k was consumed deviation_from_optimal_nutrients \u0026lt;- log(optimal_ratio_numeric/ potential_nutrient_ratio_numeric) # Set probability to select item_k according to how much the nutrients # deviate from the optimal amount. Note that the probabilities # for all different items don\u0026#39;t have to sum up to 1 to be # used in `sample()`, they are rather considered as relative weights. # If deviation \u0026gt; threshold, set value to 0 if(abs(deviation_from_optimal_nutrients) \u0026gt; threshold_deviation){ probability_weight_choosing_item[item_k] \u0026lt;- 0 } else { # Explanation in text, but in short: if deviation is 0, just use food # availability as probability. With increasing deviation, put a larger # penalty on item and decrease probability accordingly.The severity of # the penalty is dependent on the threshold of maximum \u0026quot;allowed\u0026quot; deviation probability_weight_choosing_item[item_k] \u0026lt;- (1 - (deviation_from_optimal_nutrients/threshold_deviation)^2) * availability_per_individual[[item_k, individual_j]] } } # Based on probability/weight, select one of the food items item_selected \u0026lt;- sample(x = 1:n_items, size = 1, prob = probability_weight_choosing_item) # Add the nutrients of the selected item to current nutrients current_nutrients_per_individual[[individual_j]] \u0026lt;- current_nutrients_per_individual[[individual_j]] + items_nutrients[[item_selected]] # Assign new current nutrient values to cells in array nutrients_consumed[event_i, individual_j, ] \u0026lt;- current_nutrients_per_individual[[individual_j]] # Store information about which item was included in the diet in a matrix items_consumed[event_i, individual_j] \u0026lt;- item_selected } } # Finally, return object according to return_class if(return_class == \u0026quot;list\u0026quot;){ geom_frame_list \u0026lt;- list(nutrients_consumed, items_consumed) names(geom_frame_list) \u0026lt;- c(\u0026quot;Consumed Nutrient Units\u0026quot;, \u0026quot;Items in Diet\u0026quot;) return(geom_frame_list) }else if(return_class == \u0026quot;df\u0026quot;){ nutrients_consumed_df \u0026lt;- as.data.frame(as.table(nutrients_consumed[,,1])) names(nutrients_consumed_df) \u0026lt;- c(\u0026quot;Feeding_event\u0026quot;, \u0026quot;Individual\u0026quot;, \u0026quot;Nutrient1\u0026quot;) nutrients_consumed_df$Nutrient2 \u0026lt;- as.vector(nutrients_consumed[,,2]) nutrients_consumed_df$Item_in_diet \u0026lt;- as.data.frame(as.table(items_consumed))[[3]] return(nutrients_consumed_df) } } Running the simulation for different scenarios We test our function in a simple example. In a group of 10 individuals, each individual has the nutritional target of obtaining protein and carbohydrates in a 2P : 1C ratio. The three available food items in the environment have the following nutrient composition:\nitem 1’s nutritional composition is 3P:1C\nitem 2’s nutritional composition is 1P:3C\nitem 3’s nutritional composition is 1P:1C\nTo explore how the nutritional acquisition happens depending on limited resources, the individuals are “provided” with these three items in varying levels of availability in 3 different scenarios:\nScenario: All items are equally scarce.\nScenario: Only item 1 (3P:1C) is highly abundant, the remaining items are scarce.\nScenario: Only item 2 (1P:3C) is highly abundant, the remaining items are scarce.\nPreparation to run the simulation ## setup for function items_nutrients \u0026lt;- list(c(3,1), c(1,3), c(1,1)) optimal_ratio \u0026lt;- c(2,1) threshold_deviation \u0026lt;- 1 n_feeding_events \u0026lt;- 150 n_individuals \u0026lt;- 10 initial_nutrients \u0026lt;- c(0,0) return_class \u0026lt;- \u0026quot;df\u0026quot; ## setup for plotting library(ggplot2) theme_set(theme_minimal()) optimal_carbohydrates \u0026lt;- 1:200 optimal_protein \u0026lt;- 2* optimal_carbohydrates Scenario 1: All items equally scarce In this scenario we set each item to the same, scarce (0.2) availability value.\nitems_availability_scenario1 \u0026lt;- c(0.2, 0.2, 0.2) gf_df1 \u0026lt;- geometric_framework(items_nutrients = items_nutrients, items_availability = items_availability_scenario1, optimal_ratio = optimal_ratio, threshold_deviation = threshold_deviation, n_feeding_events = n_feeding_events, n_individuals = n_individuals, initial_nutrients = initial_nutrients, return_class = return_class) head(gf_df1) ## Feeding_event Individual Nutrient1 Nutrient2 Item_in_diet ## 1 1 1 1 1 3 ## 2 2 1 2 2 3 ## 3 3 1 5 3 1 ## 4 4 1 6 4 3 ## 5 5 1 9 5 1 ## 6 6 1 10 8 2 We illustrate the nutrient acquisition over feeding events with equally highly available items in the individuals environment.\nggplot() + geom_point(aes(x = Nutrient1, y = Nutrient2, color = Individual), alpha = 0.7, data = gf_df1) + geom_line(aes(x = optimal_protein, y = optimal_carbohydrates)) + ylim(0, 200) + theme(legend.position = \u0026quot;none\u0026quot;) + labs(title = \u0026quot;Nutrient acquisition when all items are equally scarce\u0026quot;, x = \u0026quot;Units Proteins\u0026quot;, y = \u0026quot;Units Carbohydrates\u0026quot;) Figure 5: Scenario 1 (items are equally scare) nutrient acquisition. The plot shows the acquisition of proteins on the x-axis in relation to the acquisition of carbohydrates on the y-axis over feeding events. The individuals nutrient acquisition is highlighted as dots. Different colors distinguish the simulated individuals. The nutritional goal of the individuals is highlighted as black line Figure 5 illustrates that the cumulative nutrient acquisition of our simulated individuals deviates from the target ratio when all items are equally scarce in the environment. The nutritional goal of our simulated animals is to consume 2P:1C. We included two items (item 2 with 1P:3C and item 3 with 1P:1C) that would lead to a higher carbohydrate consumption compared to the nutrient target, and one item (item 1 with a 3P:1C) that would lead to a higher protein consumption compared to the nutrient target.\nWhen animals experience food scarcity they are not able to maintain their target ratio, but still consume more units proteins than expected by the relative availability. Especially in the first few feeding events, when the total nutrient acquisition is still low, they stay closer to the target ratio compared to later feeding events when already many nutrients have been consumed. Individuals become less selective the more total nutrients they have already consumed.\nScenario 2: Only item 1 abundant In this scenario we set Item 1 (3P:1C) to a high availability of 0.8, whereas item 2 (1P:3C) and item 3 (1P:1C) are both set to a low availability of 0.2.\nitems_availability_scenario2 \u0026lt;- c(0.8, 0.2, 0.2) gf_df2 \u0026lt;- geometric_framework(items_nutrients = items_nutrients, items_availability = items_availability_scenario2, optimal_ratio = optimal_ratio, threshold_deviation = threshold_deviation, n_feeding_events = n_feeding_events, n_individuals = n_individuals, initial_nutrients = initial_nutrients, return_class = return_class) head(gf_df2) ## Feeding_event Individual Nutrient1 Nutrient2 Item_in_diet ## 1 1 1 3 1 1 ## 2 2 1 6 2 1 ## 3 3 1 9 3 1 ## 4 4 1 12 4 1 ## 5 5 1 15 5 1 ## 6 6 1 18 6 1 We illustrate the nutrient acquisition over feeding events with only item 1, which has a high protein content and is abundant in the individuals’ environment. The remaining items are scarce.\nggplot() + geom_point(aes(x = Nutrient1, y = Nutrient2, color = Individual), alpha = 0.7, data = gf_df2) + geom_line(aes(x = optimal_protein, y = optimal_carbohydrates)) + ylim(0, 200) + theme(legend.position = \u0026quot;none\u0026quot;) + labs(title = \u0026quot;Nutrient acquisition when only item 1 (3P:1C) is abundant\u0026quot;, x = \u0026quot;Units Proteins\u0026quot;, y = \u0026quot;Units Carbohydrates\u0026quot;) Figure 6: Scenario 2 (only item 1 (3P:1C) is abundant) nutrient acquisition. The plot shows the acquisition of proteins on the x-axis in relation to the acquisition of carbohydrates on the y-axis over feeding events. The individuals nutrient acquisition is highlighted as dots. Different colors distinguish the simulated individuals. The nutritional goal of the individuals is highlighted as black line Figre 6 illustrates that the cumulative nutrient acquisition of our simulated individuals deviates from the target ratio when only item 1, which provides the highest ratio of proteins compared to carbohydrates (3P:1C), is abundant in the environment. The nutritional goal of our simulated animals is to consume proteins and carbohydrates in a 2 to 1 ratio. We included two items (item 2 with a 1P:3C and item 3 with 1P:1C) that would lead to a higher carbohydrate consumption compared to the nutrient target, and one item (item 1 with a 3P:1C ) that would lead to a higher protein consumption compared to the nutrient target.\nIf animals are provided with enough protein sources (item 1 abundant) they maintain their target ratio. Especially in the first few feeding events, when the total nutrient acquisition is still low, staying close to the target ratio is more important than in later feeding events when already many nutrients have been consumed. As before, individuals become less picky the more they have consumed.\nScenario 3: Only item 2 abundant In this scenario we set Item 2 (1P:3C) to a high availability of 0.8, whereas item 1 (3P:1C) and item 3 (1P:1C) are both set to a low availability of 0.2.\nitems_availability_scenario3 \u0026lt;- c(0.2, 0.8, 0.2) gf_df3 \u0026lt;- geometric_framework(items_nutrients = items_nutrients, items_availability = items_availability_scenario3, optimal_ratio = optimal_ratio, threshold_deviation = threshold_deviation, n_feeding_events = n_feeding_events, n_individuals = n_individuals, initial_nutrients = initial_nutrients, return_class = return_class) head(gf_df3) ## Feeding_event Individual Nutrient1 Nutrient2 Item_in_diet ## 1 1 1 1 1 3 ## 2 2 1 2 2 3 ## 3 3 1 5 3 1 ## 4 4 1 6 4 3 ## 5 5 1 7 7 2 ## 6 6 1 8 10 2 We illustrate the nutrient acquisition over feeding events with only item 2, which has a high carbohydrate content and is abundant in the individuals’ environment. The remaining items are scarce.\nggplot() + geom_point(aes(x = Nutrient1, y = Nutrient2, color = Individual), alpha = 0.7, data = gf_df3) + geom_line(aes(x = optimal_protein, y = optimal_carbohydrates)) + ylim(0, 200) + theme(legend.position = \u0026quot;none\u0026quot;) + labs(title = \u0026quot;Nutrient acquisition when only item 2 (1P:3C) is abundant\u0026quot;, x = \u0026quot;Units Proteins\u0026quot;, y = \u0026quot;Units Carbohydrates\u0026quot;) Figure 7: Scenario 3 (only item 2 (1P:3C) is abundant) nutrient acquisition. The plot shows the acquisition of proteins on the x-axis in relation to the acquisition of carbohydrates on the y-axis over feeding events. The individuals nutrient acquisition is highlighted as dots. Different colors distinguish the simulated individuals. The nutritional goal of the individuals is highlighted as black line Figure 7 illustrates that the cumulative nutrient acquisition of our simulated individuals deviates from the target ratio when only item 1, which provides the highest ratio of proteins compared to carbohydrates, is abundant in the environment.The nutritional goal of our simulated animals is to consume proteins and carbohydrates in a 2 to 1 ratio. We included two items (item 2 with a 1P:3C and item 3 with 1P:1C) that would lead to a higher carbohydrate consumption compared to the nutrient target, and one item (item 1 with 3P:1C) that would lead to a higher protein consumption compared to the nutrient target.\nIf animals are limited in their protein source (item 1 scarce) and primarily encounter carbohydrates while moving through their nutritional landscape (only item 2 abundant) they can’t maintain their target ratio and over consume carbohydrates.\nSummary In this example, we only included 3 items and simulated animals that aim to follow a nutritional ratio of 2P:1C. We included two items (item 2 with 1P:3C and item 3 with 1P:1C ) that would lead to a higher carbohydrate consumption compared to the nutrient target, and one item (item 1 with 3P:1C) that would lead to a higher protein consumption compared to the nutrient target. Under our set conditions, the animals can only maintain their target ratio when item 1 is more abundant than the other items (Scenario 2). The animals deviate from the target ratio when there is protein scarcity (Scenario 1) and especially when they primarily encounter items with different nutritional signatures, they may reach their limiting deviation to the optimal nutrient ratio (Scenario 3), while moving through their nutritional landscape.\nIn Part 2 we explore how well commonly used preference measures (namely electivity indices) can capture the preference of animals following Geometric Framework Models for different food items and how preference might change depending on resource limitations.\nLambert, J. E., \u0026amp; Rothman, J. M. (2015). Fallback Foods, Optimal Diets, and Nutritional Targets: Primate Responses to Varying Food Availability and Quality. Annual Review of Anthropology, 44(1), 493–512. https://doi.org/10.1146/annurev-anthro-102313-025928 Manly, B. F. J., McDonald, L. L., Thomas, D. L., McDonald, T. L., \u0026amp; Erickson, W. P. (2007). Resource Selection by Animals. 231. Simpson, S. J., \u0026amp; Raubenheimer, D. (2011). The nature of nutrition: a unifying framework. Australian Journal of Zoology, 59(6), 350. https://doi.org/10.1071/ZO11068 ","date":1689984000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1689984000,"objectID":"1feaa3bffa5f9685157c2753d3c90532","permalink":"https://bedatablog.netlify.com/post/food-simulation-i/","publishdate":"2023-07-22T00:00:00Z","relpermalink":"/post/food-simulation-i/","section":"post","summary":"Conception, Idea, and Writing: Patrick Lauer and Urs Kalbitzer. Realization and Programming: Patrick Lauer.\nIntroduction Animals forage to meet their nutritional needs while avoiding potentially harmful components, such as toxins. Geometric Framework Models, which have gained popularity in recent years, provide a framework that takes into account that there is rarely one single resource that satisfies an animal’s nutritional requirements and animals have to balance the consumption of different food resources with different nutritional compositions (Lambert \u0026amp; Rothman, 2015; Simpson \u0026amp; Raubenheimer, 2011).","tags":["R"],"title":"Foraging Simulation following Geometric Framework Models  - Part 1","type":"post"},{"authors":null,"categories":[],"content":" Current climate data, and projections how climate may change in the (near) future, are important for various reasons. For example, such data can be used to predict how habitat suitability for different animal and plant species or agricultural productivity changes over time.\nThus, in this post, I am showing how to get data on recent and projected climate directly into R, crop the obtained object to the area of interest (here: South America), and then calculate and illustrate the projected change. These data can then be used for further analyses in R.\nThe data are taken from worldclim.org, which provides climate projections from global climate models (GCM) following the Coupled Model Intercomparison Projects 5 (CMIP5) protocol. These models were used for the 5th report of the Intergovernmental Panel on Climate Change (IPCC) published in 2014. New models are currently developed following CMIP6, and will be used for the 6th IPCC report. I hope/guess that there will be soon an easy way to get these newer predictions into R (please leave a comment if you know about such a way).\nFor the GCMs, there are four representative concentration pathways (RCPs) describing different climate futures depending on the emitted volumes of greenhouse gases. I am using RCP 4.5 here, which assumes a peak decline in green house gases around 2040, followed by decreasing emission. This is a rather optimistic scenario.\nHere are the different steps:\nUse the getData function from the raster package to get climate data into R. Get a map of South America with the rnaturalearth and sf packages. Crop the climate data to South America and calculate projected changes with the stars package. Use the ggplot and patchwork packages to illustrate the changes. Some additional resources:\nVignettes of the stars package Vignettes of the sf package Online book on Geocomputation with R by Lovelace, Nowosad, and Muenchow 1. Get climate data with raster::getData 1.1 Prepare R rm(list = ls()) library(stars) # To process the raster data library(sf) # To work with vector data library(ggplot2) # For plotting library(patchwork) # To combine different ggplot plots Additional packages that are used: raster (to get the climate data), rnaturalearth (to get the map of South America).\n1.2 Get recent climate data The getData function from the raster package makes it possible to easily download data on past, current/recent, and projected climate (and some other global geographic data sets. See ?raster::getData for details).\nHere, I am downloading interpolations of observed data representative for the period 1960-1990 (it is also possible to get data for the period 1970-2000). To do so, I am using the following arguments:\nname = 'worldclim' to download data from worldclim.org. var = 'bio' to get annual averages for all available climate variables. Other possibilities are, e.g., ‘tmin’ or ‘tmax’ for monthly minimum and maximum temperature. res = 10 for the resolution of 10 minutes of degree. This downloads the global data set. For higher resolutions (e.g., 2.5), the tile(s) have to be specified (with lon and lat). path specifies that files are downloaded into subfolder ‘/blog_data’. For this dataset, the files ‘bio1.bil’ to ‘bio19.bil’ (plus ‘.hdr’ files) will be downloaded to ‘/blog_data/wc10/’. file_path \u0026lt;- paste0(dirname(here::here()), \u0026quot;/blog_data/\u0026quot;) raster::getData(name = \u0026#39;worldclim\u0026#39;, var = \u0026#39;bio\u0026#39;, res = 10, path = file_path) ## class : RasterStack ## dimensions : 900, 2160, 1944000, 19 (nrow, ncol, ncell, nlayers) ## resolution : 0.1666667, 0.1666667 (x, y) ## extent : -180, 180, -60, 90 (xmin, xmax, ymin, ymax) ## crs : +proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0 ## names : bio1, bio2, bio3, bio4, bio5, bio6, bio7, bio8, bio9, bio10, bio11, bio12, bio13, bio14, bio15, ... ## min values : -269, 9, 8, 72, -59, -547, 53, -251, -450, -97, -488, 0, 0, 0, 0, ... ## max values : 314, 211, 95, 22673, 489, 258, 725, 375, 364, 380, 289, 9916, 2088, 652, 261, ... 1.3 Get projected climate data To get climate data projected for the period 2061-2080, I am using the following arguments:\nname = 'CMIP5' to get data from the CMIP5 models. var = 'bio', which includes the same 19 variables for annual averages as for ‘worldclim’ (the set of variables other than ‘bio’ is very limited for ‘CMIP5’). res = 10 as above. rcp = 45 (see introduction) model = 'IP' (the ‘IPSL-CM5A-LR’ model). Check ?raster::getData for a list of all models. Perhaps, downloading all/a subset of models and then average projections is better than using a single model, but for the sake of simplicity, I only download this single model here. year = 70 to get projections for the period 2061-2080 (alternative is 50). path as above. Files ‘ip45bi701.tif’- ‘ip45bi7019.tif’ will be downloaded to ‘/blog_data/cmip5/10m/’. raster::getData(name = \u0026#39;CMIP5\u0026#39;, var = \u0026#39;bio\u0026#39;, res = 10, rcp = 45, model = \u0026#39;IP\u0026#39;, year = 70, path = file_path) ## class : RasterStack ## dimensions : 900, 2160, 1944000, 19 (nrow, ncol, ncell, nlayers) ## resolution : 0.1666667, 0.1666667 (x, y) ## extent : -180, 180, -60, 90 (xmin, xmax, ymin, ymax) ## crs : +proj=longlat +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +no_defs ## names : ip45bi701, ip45bi702, ip45bi703, ip45bi704, ip45bi705, ip45bi706, ip45bi707, ip45bi708, ip45bi709, ip45bi7010, ip45bi7011, ip45bi7012, ip45bi7013, ip45bi7014, ip45bi7015, ... ## min values : -227, -60, -80, 86, -22, -512, 53, -215, -421, -60, -455, 0, 0, 0, 0, ... ## max values : 344, 217, 94, 22678, 524, 283, 729, 415, 401, 421, 320, 10541, 2701, 489, 222, ... 1.4 Load and process temperature data The first variable from ‘bio’ is annual average temperature in 10 * °C. For recent data, the file name for this variable is ‘bio1.bil’, and for projected data ‘ip45bi701.tif’. Both can be loaded with the read_stars function from the stars package. Values have to be divided by 10 to get the temperature in °C (instead of 10 * °C)\nannual_T \u0026lt;- stars::read_stars(paste0(file_path, \u0026quot;wc10/bio1.bil\u0026quot;)) annual_T \u0026lt;- annual_T/10 annual_T_70 \u0026lt;- stars::read_stars(paste0(file_path, \u0026quot;cmip5/10m/ip45bi701.tif\u0026quot;)) annual_T_70 \u0026lt;- annual_T_70/10 1.5 Quick Plots For the plots, I am defining a color palette for temperature. Colors are taken from the “5-class RdYlBu” palette from http://colorbrewer2.org.\n# The result, temp_colors, is a function with argument n for the number of # colors. temp_colors \u0026lt;- colorRampPalette(c(\u0026quot;#2c7bb6\u0026quot;, \u0026quot;#abd9e9\u0026quot;, \u0026quot;#ffffbf\u0026quot;, \u0026quot;#fdae61\u0026quot;, \u0026quot;#d7191c\u0026quot;)) Then, global maps can be plotted with this color palette:\nnbreaks \u0026lt;- 20 { par(mfrow = c(1,2)) plot(annual_T, main = \u0026quot;Annual temperature - 1960-1990\u0026quot;, nbreaks = nbreaks, col = temp_colors(nbreaks - 1)) plot(annual_T_70, main = \u0026quot;Annual temperature - RCP 4.5 projection for 2061-2080\u0026quot;, nbreaks = nbreaks, col = temp_colors(nbreaks - 1)) } Colors are not directly comparable between the two maps! The temperature range of each data set is used to define the color range for each map. Thus, in each map, the bluest color indicates the coolest temperature and the reddest color the hottest temperature, which differ between the two maps. This will be fixed in the plot below.\n2. Get Map of South America Get all countries from South America with the rnaturalearth package, and then combine them to a single shape.\nsouth_america_map \u0026lt;- rnaturalearth::ne_countries(continent = \u0026quot;south america\u0026quot;, returnclass = \u0026quot;sf\u0026quot;) # The precision has to be set to a value \u0026gt; 0 to resolve internal boundaries. st_precision(south_america_map) \u0026lt;- 1e9 # Required to south_america_map \u0026lt;- st_union(south_america_map) { par(mar = c(0,0,0,0)) plot(south_america_map) } 3. Climate of South America 3.1 Crop climate raster data to area of South America To crop a raster with the stars package, square brackets [] will work as crop operator (see here for details).\nannual_T_SA \u0026lt;- annual_T[south_america_map] # CRS for projected T and south america map are the same (EPSG 4326) but the # proj4string includes more details for annual_T_70. Thus, they have to # be made identical before cropping. st_crs(annual_T_70) \u0026lt;- st_crs(south_america_map) annual_T_70_SA \u0026lt;- annual_T_70[south_america_map] Quick plots:\n{ par(mfrow = c(1, 2)) plot(annual_T_SA, main = \u0026quot;Annual temperature - 1960-1990\u0026quot;, nbreaks = nbreaks, col = temp_colors(nbreaks - 1)) plot(main = \u0026quot;Annual temperature - RCP 4.5 projection for 2061-2080\u0026quot;, annual_T_70_SA, nbreaks = nbreaks, col = temp_colors(nbreaks - 1)) } 3.2 Get some basic summaries The print method for stars objects provides some summary statistics, such as max, min, or median temperature.\nannual_T_SA ## stars object with 2 dimensions and 1 attribute ## attribute(s): ## bio1.bil ## Min. :-6.50 ## 1st Qu.:17.60 ## Median :24.00 ## Mean :21.03 ## 3rd Qu.:26.00 ## Max. :29.00 ## NA\u0026#39;s :59262 ## dimension(s): ## from to offset delta refsys point values ## x 592 872 -180 0.166667 +proj=longlat +datum=WGS8... NA NULL [x] ## y 466 874 90 -0.166667 +proj=longlat +datum=WGS8... NA NULL [y] But these values can also be manually calculated. For example, the mean temperature in South America for the period 1960 - 1990 is:\nmean(annual_T$bio1.bil, na.rm = T) ## [1] 8.144672 Currently, bio1.bil is the only attribute (i.e., the variable with recent annual temperature) in the annual_T object, but stars objects can also include several attributes.\nHere, I will rename this attribute to recent. Then I will add the projected temperature from the other object (annual_T_70_SA$ip45bi701.tif) as a second attribute projected. Finally, I will calculate the difference between the two, which is the projected change.\nnames(annual_T_SA) \u0026lt;- \u0026quot;recent\u0026quot; annual_T_SA$projected \u0026lt;- annual_T_70_SA$ip45bi701.tif annual_T_SA$change \u0026lt;- annual_T_SA$projected - annual_T_SA$recent annual_T_SA ## stars object with 2 dimensions and 3 attributes ## attribute(s): ## recent projected change ## Min. :-6.50 Min. :-4.00 Min. :1.00 ## 1st Qu.:17.60 1st Qu.:20.10 1st Qu.:2.50 ## Median :24.00 Median :26.80 Median :2.80 ## Mean :21.03 Mean :23.76 Mean :2.73 ## 3rd Qu.:26.00 3rd Qu.:28.80 3rd Qu.:3.00 ## Max. :29.00 Max. :31.70 Max. :4.40 ## NA\u0026#39;s :59262 NA\u0026#39;s :59262 NA\u0026#39;s :59262 ## dimension(s): ## from to offset delta refsys point values ## x 592 872 -180 0.166667 +proj=longlat +datum=WGS8... NA NULL [x] ## y 466 874 90 -0.166667 +proj=longlat +datum=WGS8... NA NULL [y] 4. Illustrate changes in temperature with ggplot and patchwork Here, we can use scale_fill_gradientn to define temperature colors, and then use the same limits for both plots. Now, colors are directly comparable between the plots for annual temperature. I will only use red colors for the change in temperature, as these values are always positive.\nrecent_T_plot \u0026lt;- ggplot() + geom_stars(data = annual_T_SA) + scale_fill_gradientn(name = \u0026quot;Annual T [°C]\u0026quot;, colors = temp_colors(5), limits = c(-7, 32), na.value = \u0026quot;white\u0026quot;) + geom_sf(data = south_america_map, fill = NA) + scale_x_discrete(expand = c(0, 0)) + scale_y_discrete(expand = c(0, 0)) + ggtitle(\u0026quot;a) 1960-1990\u0026quot;) + theme_void() + theme(legend.position = \u0026quot;none\u0026quot;) projected_T_plot \u0026lt;- ggplot() + geom_stars(data = annual_T_SA[\u0026quot;projected\u0026quot;]) + scale_fill_gradientn(name = \u0026quot;Annual T [°C]\u0026quot;, colors = temp_colors(5), limits = c(-7, 32), na.value = \u0026quot;white\u0026quot;) + geom_sf(data = south_america_map, fill = NA) + scale_x_discrete(expand = c(0, 0)) + scale_y_discrete(expand = c(0, 0)) + ggtitle(\u0026quot;b) 2061-2080 (projected)\u0026quot;) + theme_void() + theme(legend.position = \u0026quot;bottom\u0026quot;) projected_change_T_plot \u0026lt;- ggplot() + geom_stars(data = annual_T_SA[\u0026quot;change\u0026quot;]) + scale_fill_gradientn(name = \u0026quot;Change in T [°C]\u0026quot;, colors = temp_colors(5)[3:5], limits = c(1, 5), na.value = \u0026quot;white\u0026quot;) + geom_sf(data = south_america_map, fill = NA) + scale_x_discrete(expand = c(0, 0)) + scale_y_discrete(expand = c(0, 0)) + ggtitle(\u0026quot;c) Projected change\u0026quot;) + theme_void() + theme(legend.position = \u0026quot;bottom\u0026quot;) Finally, I will combine the three maps with the patchwork package (see here for details).\n(recent_T_plot / projected_T_plot + plot_layout(guides = \u0026quot;keep\u0026quot;)) | projected_change_T_plot + theme(plot.margin = margin(c(0, 0, 0, 0))) ","date":1580342400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580427533,"objectID":"9a819ff6ab97f73b4d944d416cb6a952","permalink":"https://bedatablog.netlify.com/post/download-and-illustrate-current-and-projected-climate-in-r/","publishdate":"2020-01-30T00:00:00Z","relpermalink":"/post/download-and-illustrate-current-and-projected-climate-in-r/","section":"post","summary":"Current climate data, and projections how climate may change in the (near) future, are important for various reasons. For example, such data can be used to predict how habitat suitability for different animal and plant species or agricultural productivity changes over time.\nThus, in this post, I am showing how to get data on recent and projected climate directly into R, crop the obtained object to the area of interest (here: South America), and then calculate and illustrate the projected change.","tags":["climate","raster","stars","sf","ggplot","patchwork","rnaturalearth","South America"],"title":"Download and illustrate current and projected climate data in R","type":"post"},{"authors":null,"categories":null,"content":" Interested in the distribution of animal species on a global or continental level? This can be easily illustrated with R using the data provided by the IUCN Red List. In this post, I will focus on mammals in Africa, the continent where I have conducted most of my field work. But the code below can be easily adapted to other areas of the world and other taxonomic groups of species, or only species at low or high risk of extinction (though the completeness of the IUCN data is variable across different groups).\nIn this post, I describe the following steps:\nDownload and process the IUCN Red List spatial data set for terrestrial mammals Download a map of Africa Create a hexagon grid for Africa Combine species ranges with the grid and plot the map The processing and summarizing of the spatial data is mostly done with the impressive sf package. For my first attempts to work with spatial data in R (more specifically with vector data), I used the sp package. But the sf package, which is the successor of the sp, is much easier to use. Additionally, it integrates some of the tidyverse functions, most importantly many of the dplyr functions, which so many R users are already using for other purposes. Also, the sf package is very well documented and the vignettes are extremely helpful. They can be found on the github page of the package or by typing vignette(package = \u0026quot;sf\u0026quot;). I also would like to mention the book ‘Geocomputation with R’, which is a very informative (open source) resource for anybody interested in working with spatial data in R.\n1. Download and process the IUCN Red List spatial data set for terrestrial mammals 1.1 Prepare R rm(list = ls()) library(tidyverse) library(sf) 1.2 Download and process the IUCN terrestial mammals shapefile The IUCN Red List spatial data can be obtained here. The IUCN datasets are freely available for non-commercial use, but they have to be downloaded manually because you have to register for an account and provide a description of the intended usage. For academic use, you can (usually) download the dataset immediately after you made your request.\nThe ranges of populations and species are all provided as polygons in the downloadable shapefiles. However, some species might be missing, and some ranges might be incomplete. Nevertheless, it is an impressive data set, and at least for primates, it looks fairly complete. Further details (and limitations) can be found on the IUCN spatial data webpage.\nFor this post, I downloaded the “Terrestrial Mammal” datasets, which is about ~ 600 MB. Then, I moved the downloaded folder (‘TERRESTRIAL_MAMMALS’) into the folder “~/Sync/iucn_data/” on my harddrive and renamed it to ‘TERRESTRIAL_MAMMALS_2020_01_11’.\nI split up the following process into two steps: 1) load, simplify (i.e., reduce the size of the object), and save the processed shapefile a .gpkg file; and 2) re-load the processed files. The first step will not be evaluated in the final notebook so that the 600 MB shapefile does not have to be processed every time I run this notebook.\nmammals \u0026lt;- st_read(\u0026quot;~/Sync/iucn_data/TERRESTRIAL_MAMMALS_2020_01_11\u0026quot;) # Simplify the polygons to reduce the size of the object/file. This may take a # few moments but the size of the object (in the R environment) changes from 906 # Mb to \u0026lt;10 Mb (with dTolerance = 5), which is much easier to handle. mammals_simple \u0026lt;- mammals %\u0026gt;% st_simplify(dTolerance = 1) mammals_simple \u0026lt;- mammals_simple %\u0026gt;% select(binomial, presence,legend, category) # Write the object to a file and clean up. I use .gpkg extension here because it # just requires one file instead of several files created for shapefiles. Also, # it\u0026#39;s an open format and it appears to be just as good as shapefiles (at least # for everything I\u0026#39;ve done so far). Look here for more information: # https://www.gis-blog.com/geopackage-vs-shapefile/ file_name \u0026lt;- paste0(dirname(here::here()), \u0026quot;/blog_data/iucn_mammal_distribution.gpkg\u0026quot;) st_write(mammals_simple, file_name) rm(list = c(\u0026quot;mammals\u0026quot;, \u0026quot;mammals_simple\u0026quot;, \u0026quot;file_name\u0026quot;)) Note: I set the dTolerance argument to 1, but for a more precise map, it might be good to change it to 0.1 or a lower value.\nAlso, I only kept four of the columns: binomial (the species name), presence and legend (indicating whether a population is still present), and category (the risk category for each species). For other projects, additional columns can be added. For example, order_ or family indicate the taxonomic order/family of a species.\nNow, load the file (step 2)\nfile_name \u0026lt;- paste0(dirname(here::here()), \u0026quot;/blog_data/iucn_mammal_distribution.gpkg\u0026quot;) mammals \u0026lt;- st_read(file_name) ## Reading layer `iucn_mammal_distribution\u0026#39; from data source `/Users/urs/Google Drive/Data Blog/blog_data/iucn_mammal_distribution.gpkg\u0026#39; using driver `GPKG\u0026#39; ## Simple feature collection with 12908 features and 4 fields (with 7579 geometries empty) ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -179.999 ymin: -56.04993 xmax: 179.999 ymax: 89.9 ## epsg (SRID): 4326 ## proj4string: +proj=longlat +datum=WGS84 +no_defs Quick look at the sf object:\nhead(mammals) ## Simple feature collection with 6 features and 4 fields (with 2 geometries empty) ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -58.02031 ymin: -43.29721 xmax: 153.0728 ymax: 50.38943 ## epsg (SRID): 4326 ## proj4string: +proj=longlat +datum=WGS84 +no_defs ## binomial presence legend category ## 1 Nyctereutes procyonoides 1 Extant (resident) LC ## 2 Rattus lutreolus 1 Extant (resident) LC ## 3 Rhinolophus subrufus 1 Extant (resident) DD ## 4 Mops spurrelli 1 Extant (resident) LC ## 5 Clyomys laticeps 1 Extant (resident) LC ## 6 Eonycteris spelaea 1 Extant (resident) LC ## geom ## 1 MULTIPOLYGON (((143.7874 50... ## 2 MULTIPOLYGON (((147.0231 -3... ## 3 MULTIPOLYGON EMPTY ## 4 MULTIPOLYGON (((16.12875 4.... ## 5 MULTIPOLYGON (((-47.53412 -... ## 6 MULTIPOLYGON EMPTY For an explanation of sf objects, look at the first vignette of the sf package. Every row in this data frame represents one population of species binomial, with additional information about the presence and risk category (columns presence, legend, and category). The sf-column geom contains the geographic information describing the range of each population. As far as I understand this data set, several populations might be combined into one row (the sf-column can contain several polygons), but in other cases, each population has its own row.\nWe can simply plot all polygons (for all rows) with:\n{ par(mar = c(0, 0, 0, 0)) plot(st_geometry(mammals)) } This plot shows the distributions of all mammal species/populations, but it also includes populations that are no longer present. Let’s have a look at the different codes for presence and the description, which can be found in the column legend.\nmammals %\u0026gt;% st_set_geometry(NULL) %\u0026gt;% distinct(presence, legend) %\u0026gt;% arrange(presence) ## presence legend ## 1 1 Extant (resident) ## 2 1 Extant \u0026amp; Introduced (resident) ## 3 1 Extant \u0026amp; Reintroduced (resident) ## 4 1 Extant \u0026amp; Vagrant (seasonality uncertain) ## 5 1 Extant \u0026amp; Origin Uncertain (resident) ## 6 1 Extant (seasonality uncertain) ## 7 1 Extant (non-breeding) ## 8 1 Extant \u0026amp; Vagrant (resident) ## 9 2 Probably Extant (resident) ## 10 2 Probably Extant \u0026amp; Origin Uncertain (resident) ## 11 2 Probably Extant \u0026amp; Introduced (resident) ## 12 3 Possibly Extant (resident) ## 13 3 Possibly Extant \u0026amp; Origin Uncertain (resident) ## 14 3 Possibly Extant (seasonality uncertain) ## 15 3 Possibly Extant (passage) ## 16 3 Possibly Extant \u0026amp; Vagrant (seasonality uncertain) ## 17 3 Possibly Extant \u0026amp; Introduced (resident) ## 18 3 Possibly Extant \u0026amp; Vagrant (non-breeding) ## 19 4 Possibly Extinct ## 20 4 Possibly Extinct \u0026amp; Introduced ## 21 5 Extinct ## 22 5 Extinct \u0026amp; Origin Uncertain ## 23 5 Extinct \u0026amp; Introduced ## 24 5 Extinct \u0026amp; Reintroduced ## 25 6 Presence Uncertain ## 26 6 Presence Uncertain \u0026amp; Origin Uncertain ## 27 6 Presence Uncertain \u0026amp; Vagrant Codes/numbers 1, 2, and 3 refer to ‘extant’, ‘probably extant’, and ‘possibly extant’ populations, and I only keep these populations here.\nmammals_extant \u0026lt;- mammals %\u0026gt;% filter(presence %in% c(1,2,3)) 2. Download a map of Africa In R, it’s very easy to get maps of countries or continents from https://www.naturalearthdata.com using the rnaturalearth package. With the ne_countries() function, all countries (as polygons) for the specified continent can be downloaded. Here, I am only interested in a map of the whole continent, not single countries. This can be achieved with the dplyr::summarize() function, which ‘summarizes’ the polygons in all rows in the sf object (the countries) to one polygon (the continent). One of the examples showing how nicely the tidyverse functionality is integrated into the sf package!\nafrica_map \u0026lt;- rnaturalearth::ne_countries(continent = \u0026quot;Africa\u0026quot;, returnclass = \u0026quot;sf\u0026quot;) %\u0026gt;% st_set_precision(1e9) %\u0026gt;% summarize { par(mar = c(0, 0, 0, 0)) plot(st_geometry(africa_map)) } 3. Create a hexagon grid for Africa Since I want to illustrate how species richness varies across regions, the number of species have to be summarized for some kind of (spatial) subsets of Africa. For example, this could be done by country. But here, I use a grid of equally sized hexagon cells to illustrate the varying species richness across the continent. This is possible with the sf::st_make_grid() function. The argument square = F specifies that hexagons are created instead of squares. Furthermore, I create a grid_id column, which will be required as identifier for grid cells further below.\nafrica_grid \u0026lt;- st_make_grid(africa_map, what = \u0026quot;polygons\u0026quot;, cellsize = 0.75, square = F) %\u0026gt;% st_sf() %\u0026gt;% mutate(grid_id = row_number()) Note: the cell size can be adjusted by changing the cellsize argument, but this will also affect the number of species per cell. Larger cells are more likely to intersect with more species than smaller cells.\nFor the map, I only want to keep the parts of the grid cells that are on the continent. This might be problematic in some cases because the cells at the edge of the continent are smaller than cells within the continent. But since I only summarize terrestrial mammals here, I think it might also be misleading if the cells cover non-terrestrial area without including marine mammals.\nafrica_grid_clipped \u0026lt;- st_intersection(africa_grid, africa_map) { par(mar = c(0, 0, 0, 0)) plot(africa_map$geometry, reset = F, axes = F) plot(st_geometry(africa_grid_clipped), color = \u0026quot;white\u0026quot;, add = T, border = rgb(0, 0, 0, 0.3)) } 4. Combine species ranges with the grid and plot the map 4.1 Only keep population ranges within Africa With st_intersection(), only the portion of population ranges within Africa are kept. Then, ranges are ‘summarized’ by species, which means that for species with several rows in the data frame, these rows are combined into one row with a (multi) polygon describing the entire range of this species. The use of dplyr::group_by() in combination with dplyr::summarize() with an sf object is another great example for the integration of the tidyverse functionality in the sf package.\nafrica_mammals \u0026lt;- st_intersection(mammals, africa_map) %\u0026gt;% group_by(binomial) %\u0026gt;% summarize() { par(mar = c(0, 0, 0, 0)) plot(st_geometry(africa_mammals)) } 4.2 Combine ranges with the grid The range polygons can be combined with the grid map using the sf::st_join() function. Then, the number of species is counted per grid cell using the grid_id column created above.\n# This may take a few minutes species_per_cell \u0026lt;- africa_grid_clipped %\u0026gt;% st_join(africa_mammals) species_per_cell_sums \u0026lt;- species_per_cell %\u0026gt;% group_by(grid_id) %\u0026gt;% summarize(species_n = n()) 4.3 Create the plot The standard output with plot() looks like this.\nplot(species_per_cell_sums[\u0026quot;species_n\u0026quot;]) The plot created with plot() can be customized, but I find it easier to customize plots with the ggplot2 package. Therefore, I create another plot with ggplot() using a different color palette, and applying some other modifications.\nafrican_mammals_map \u0026lt;- ggplot() + geom_sf(data = species_per_cell_sums, aes(fill = species_n), size = 0) + scale_fill_gradient2(name = \u0026quot;Number of\\nSpecies\u0026quot;, low = \u0026quot;#004529\u0026quot;, mid = \u0026quot;#f7fcb9\u0026quot;, high = \u0026quot;#7f0000\u0026quot;, midpoint = max(species_per_cell_sums$species_n)/2) + geom_sf(data = africa_map, fill = NA) + labs(title = \u0026quot;Mammal Species in Africa\u0026quot;) + theme_void() + theme(legend.position = c(0.1, 0.1), legend.justification = c(0, 0), plot.title = element_text(hjust = .5)) african_mammals_map ","date":1578700800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578884105,"objectID":"6b5f6e98623b686c8a3a19aef3deeb7b","permalink":"https://bedatablog.netlify.com/post/african-mammals-map/","publishdate":"2020-01-11T00:00:00Z","relpermalink":"/post/african-mammals-map/","section":"post","summary":"Interested in the distribution of animal species on a global or continental level? This can be easily illustrated with R using the data provided by the IUCN Red List. In this post, I will focus on mammals in Africa, the continent where I have conducted most of my field work. But the code below can be easily adapted to other areas of the world and other taxonomic groups of species, or only species at low or high risk of extinction (though the completeness of the IUCN data is variable across different groups).","tags":["IUCN","R","Red List","rnaturalearth","rpatial","sf"],"title":"African Mammals","type":"post"},{"authors":[],"categories":[],"content":" In the previous post, I simulated clustered networks with varying strength of within-group, between-close-groups, and between-group social relationships. In part 2, I am using some of the methods provided by the igraph package to test how well these simulated clusters can be (re)-detected.\n2. Determine clustering using different methods and compare the results 2.1 Prepare R rm(list = ls()) library(tidyverse) library(tidygraph) library(ggraph) library(igraph) In addition to these packages, this notebook requires the installation of the following packages: DT, cowplot, ggrepel.\n2.2 Recreate the network from previous post To avoid repetition, the code is not shown here, but can be copied from the previous post. The two objects that are required from there are individual_df and network_df.\nThe networks looked like that:\nOne important note: My motivation for this small project was my research on a group of ~50 adult red colobus monkeys. Therefore, I simulated networks where most individuals were somehow connected. The plot above, however, only shows association indices \\(\\ge\\) 0.3 to make it easier to see the clusters. The complete network is much denser (see the final plot of this post). After finalizing the draft of this post, I realized that this may have affected how well (or not) the algorithms detected the (sub)-groups and therefore wanted to mention it here.\n2.3 Use different algorithms to detect clusters The igraph-package provides several functions to detect communities within a network, which start with cluster_ (type ?communities in R for a complete list). Most of these algorithms are explained here and here.\nBefore using the cluster_ functions, the networks have to be transformed into igraph objects.\ndf_to_network \u0026lt;- function(network_df, weight_col){ network \u0026lt;- graph_from_data_frame(select(network_df, from = Ind_A, to = Ind_B, weight = matches(weight_col)), directed = FALSE) return(network) } network_1 \u0026lt;- df_to_network(network_df, weight_col = \u0026quot;S1\u0026quot;) network_2 \u0026lt;- df_to_network(network_df, weight_col = \u0026quot;S2\u0026quot;) network_3 \u0026lt;- df_to_network(network_df, weight_col = \u0026quot;S3\u0026quot;) network_4 \u0026lt;- df_to_network(network_df, weight_col = \u0026quot;S4\u0026quot;) Here’s the standard output for such an object:\nnetwork_1 ## IGRAPH 1485755 UNW- 50 1225 -- ## + attr: name (v/c), weight (e/n) ## + edges from 1485755 (vertex names): ## [1] ind_01--ind_02 ind_01--ind_03 ind_01--ind_04 ind_01--ind_05 ind_01--ind_06 ## [6] ind_01--ind_07 ind_01--ind_08 ind_01--ind_09 ind_01--ind_10 ind_01--ind_11 ## [11] ind_01--ind_12 ind_01--ind_13 ind_01--ind_14 ind_01--ind_15 ind_01--ind_16 ## [16] ind_01--ind_17 ind_01--ind_18 ind_01--ind_19 ind_01--ind_20 ind_01--ind_21 ## [21] ind_01--ind_22 ind_01--ind_23 ind_01--ind_24 ind_01--ind_25 ind_01--ind_26 ## [26] ind_01--ind_27 ind_01--ind_28 ind_01--ind_29 ind_01--ind_30 ind_01--ind_31 ## [31] ind_01--ind_32 ind_01--ind_33 ind_01--ind_34 ind_01--ind_35 ind_01--ind_36 ## [36] ind_01--ind_37 ind_01--ind_38 ind_01--ind_39 ind_01--ind_40 ind_01--ind_41 ## + ... omitted several edges And here’s the output if one of the cluster_ functions is applied to a network, in this example the fast-greedy algorithm to the first network (S1):\ncluster_fast_greedy(network_1) ## IGRAPH clustering fast greedy, groups: 3, mod: 0.32 ## + groups: ## $`1` ## [1] \u0026quot;ind_05\u0026quot; \u0026quot;ind_09\u0026quot; \u0026quot;ind_12\u0026quot; \u0026quot;ind_17\u0026quot; \u0026quot;ind_20\u0026quot; \u0026quot;ind_23\u0026quot; \u0026quot;ind_26\u0026quot; \u0026quot;ind_29\u0026quot; ## [9] \u0026quot;ind_31\u0026quot; \u0026quot;ind_32\u0026quot; \u0026quot;ind_36\u0026quot; \u0026quot;ind_38\u0026quot; \u0026quot;ind_40\u0026quot; \u0026quot;ind_43\u0026quot; \u0026quot;ind_49\u0026quot; \u0026quot;ind_50\u0026quot; ## ## $`2` ## [1] \u0026quot;ind_03\u0026quot; \u0026quot;ind_15\u0026quot; \u0026quot;ind_19\u0026quot; \u0026quot;ind_21\u0026quot; \u0026quot;ind_22\u0026quot; \u0026quot;ind_27\u0026quot; \u0026quot;ind_28\u0026quot; \u0026quot;ind_37\u0026quot; ## [9] \u0026quot;ind_39\u0026quot; \u0026quot;ind_45\u0026quot; \u0026quot;ind_46\u0026quot; \u0026quot;ind_47\u0026quot; \u0026quot;ind_48\u0026quot; ## ## $`3` ## [1] \u0026quot;ind_01\u0026quot; \u0026quot;ind_02\u0026quot; \u0026quot;ind_04\u0026quot; \u0026quot;ind_06\u0026quot; \u0026quot;ind_07\u0026quot; \u0026quot;ind_08\u0026quot; \u0026quot;ind_10\u0026quot; \u0026quot;ind_11\u0026quot; ## + ... omitted several groups/vertices Thus, the function tries to detect clusters (called groups) and assigns each individual to such a group. Furthermore, it calculates modularity (mod).\nThe aim of this post is to use several of these functions and then compare 1) the number of detected groups and 2) the detected group membership of all individuals to what was simulated (i.e., the ‘true’ values). Furthermore, I will extract the global modularity, which can be used to compare the quality of different cluster methods (many of the algorithms try to maximize modularity when searching for clusters).\nTherefore, I first define a function to apply the specified algorithm to a network, extract these metrics from the resulting communities-object, and return them in a dataframe.\nget_communities \u0026lt;- function(igraph_network, cluster_method = NULL){ if(is.null(cluster_method)) { stop(\u0026quot;no method specificied\u0026quot;) } community_object \u0026lt;- do.call(cluster_method, list(igraph_network)) return(tibble(Ind = community_object$names, Ind_Cluster = community_object$membership, global_modularity = modularity(igraph_network, community_object$membership, weights = E(igraph_network)$weight), method = cluster_method) %\u0026gt;% arrange(Ind)) } Example:\nget_communities(network_1, \u0026quot;cluster_fast_greedy\u0026quot;) ## # A tibble: 50 x 4 ## Ind Ind_Cluster global_modularity method ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 ind_01 3 0.318 cluster_fast_greedy ## 2 ind_02 3 0.318 cluster_fast_greedy ## 3 ind_03 2 0.318 cluster_fast_greedy ## 4 ind_04 3 0.318 cluster_fast_greedy ## 5 ind_05 1 0.318 cluster_fast_greedy ## 6 ind_06 3 0.318 cluster_fast_greedy ## 7 ind_07 3 0.318 cluster_fast_greedy ## 8 ind_08 3 0.318 cluster_fast_greedy ## 9 ind_09 1 0.318 cluster_fast_greedy ## 10 ind_10 3 0.318 cluster_fast_greedy ## # … with 40 more rows Using this function, I apply several of the provided igraph methods with the four networks.\nnetworks \u0026lt;- grep(\u0026quot;network_\\\\d\u0026quot;, ls(), value = T) cluster_methods \u0026lt;- c(\u0026quot;cluster_fast_greedy\u0026quot;, \u0026quot;cluster_infomap\u0026quot;, \u0026quot;cluster_label_prop\u0026quot;, \u0026quot;cluster_optimal\u0026quot;, \u0026quot;cluster_louvain\u0026quot;, \u0026quot;cluster_walktrap\u0026quot;) for(network_i in seq_along(networks)){ for(cluster_method_j in seq_along(cluster_methods)){ df_temp \u0026lt;- get_communities(igraph_network = get(networks[[network_i]]), cluster_method = cluster_methods[[cluster_method_j]]) df_temp$network \u0026lt;- networks[[network_i]] if(network_i == 1 \u0026amp; cluster_method_j == 1) community_df \u0026lt;- df_temp else community_df \u0026lt;- bind_rows(community_df, df_temp) } } # Show structure of created dataframe str(community_df) ## Classes \u0026#39;tbl_df\u0026#39;, \u0026#39;tbl\u0026#39; and \u0026#39;data.frame\u0026#39;: 1200 obs. of 5 variables: ## $ Ind : chr \u0026quot;ind_01\u0026quot; \u0026quot;ind_02\u0026quot; \u0026quot;ind_03\u0026quot; \u0026quot;ind_04\u0026quot; ... ## $ Ind_Cluster : num 3 3 2 3 1 3 3 3 1 3 ... ## $ global_modularity: num 0.318 0.318 0.318 0.318 0.318 ... ## $ method : chr \u0026quot;cluster_fast_greedy\u0026quot; \u0026quot;cluster_fast_greedy\u0026quot; \u0026quot;cluster_fast_greedy\u0026quot; \u0026quot;cluster_fast_greedy\u0026quot; ... ## $ network : chr \u0026quot;network_1\u0026quot; \u0026quot;network_1\u0026quot; \u0026quot;network_1\u0026quot; \u0026quot;network_1\u0026quot; ... The resulting table includes the global modularity for each method and network, and the estimated group membership ($Ind_Cluster) of all individuals within each networks using each of the methods.\n2.4 Compare the detected clusters with simulated groups Finally, I compare the results of the different algorithms with the simulated networks.\n# Set up dataframe for loop cluster_summary \u0026lt;- tibble( Cluster_method = rep(cluster_methods, each = length(networks)), Scenario = rep(networks, times = length(cluster_methods)), Dyads_n = length(unique(network_df$dyad)), Groups_n = length(unique(network_df$Ind_A_Group)), Clusters_n = NA, Dyads_correct_n = NA, Dyads_correct_prop = NA, Modularity = NA) # Use a loop to summarize for all methods and networks the 1) number of clusters # in comparison to simulated groups, and 2) the number of dyads correctly placed # together in a cluster. for(row_i in 1:nrow(cluster_summary)){ community_temp \u0026lt;- filter(community_df, method == cluster_summary[row_i,]$Cluster_method \u0026amp; network == cluster_summary[row_i,]$Scenario) cluster_summary_temp \u0026lt;- network_df %\u0026gt;% select(Ind_A, Ind_B, Ind_A_Group, Ind_B_Group) %\u0026gt;% left_join(select(community_temp, Ind_A = Ind, Ind_A_Cluster = Ind_Cluster), by = \u0026quot;Ind_A\u0026quot;) %\u0026gt;% left_join(select(community_temp, Ind_B = Ind, Ind_B_Cluster = Ind_Cluster), by = \u0026quot;Ind_B\u0026quot;) %\u0026gt;% mutate(same_group = (Ind_A_Group == Ind_B_Group), same_cluster = (Ind_A_Cluster == Ind_B_Cluster)) cluster_summary[row_i,]$Clusters_n = length(unique(community_temp$Ind_Cluster)) cluster_summary[row_i,]$Dyads_correct_n = sum(cluster_summary_temp$same_group == cluster_summary_temp$same_cluster) cluster_summary[row_i,]$Modularity = unique(community_temp$global_modularity) } # Calculate variables comparing simulated with detected clusters and apply some # other cosmetic changes cluster_summary \u0026lt;- cluster_summary %\u0026gt;% mutate(Dyads_correct_prop = Dyads_correct_n/Dyads_n, Clusters_vs_Groups = Clusters_n/Groups_n, Scenario = str_replace(Scenario, \u0026quot;network_\u0026quot;, \u0026quot;S\u0026quot;), Cluster_method = str_remove(Cluster_method, \u0026quot;cluster_\u0026quot;)) %\u0026gt;% mutate_if(is.numeric, ~round(., 2)) This is the resulting table:\nknitr::kable(cluster_summary) Cluster_method Scenario Dyads_n Groups_n Clusters_n Dyads_correct_n Dyads_correct_prop Modularity Clusters_vs_Groups fast_greedy S1 1225 4 3 1121 0.92 0.32 0.75 fast_greedy S2 1225 4 3 1121 0.92 0.22 0.75 fast_greedy S3 1225 4 4 1225 1.00 0.31 1.00 fast_greedy S4 1225 4 3 1121 0.92 0.22 0.75 infomap S1 1225 4 3 1121 0.92 0.32 0.75 infomap S2 1225 4 1 304 0.25 0.00 0.25 infomap S3 1225 4 1 304 0.25 0.00 0.25 infomap S4 1225 4 1 304 0.25 0.00 0.25 label_prop S1 1225 4 2 848 0.69 0.23 0.50 label_prop S2 1225 4 1 304 0.25 0.00 0.25 label_prop S3 1225 4 3 1121 0.92 0.29 0.75 label_prop S4 1225 4 2 848 0.69 0.17 0.50 optimal S1 1225 4 3 1121 0.92 0.32 0.75 optimal S2 1225 4 3 1121 0.92 0.22 0.75 optimal S3 1225 4 4 1225 1.00 0.31 1.00 optimal S4 1225 4 4 1225 1.00 0.23 1.00 louvain S1 1225 4 3 1121 0.92 0.32 0.75 louvain S2 1225 4 3 1121 0.92 0.22 0.75 louvain S3 1225 4 4 1225 1.00 0.31 1.00 louvain S4 1225 4 4 1225 1.00 0.23 1.00 walktrap S1 1225 4 3 1121 0.92 0.32 0.75 walktrap S2 1225 4 3 1121 0.92 0.22 0.75 walktrap S3 1225 4 4 1225 1.00 0.31 1.00 walktrap S4 1225 4 4 1225 1.00 0.23 1.00 Perhaps, it’s a bit easier to interpret the results when graphically illustrated.\n2.5 Illustrate results create_plot \u0026lt;- function(df, y_var, y_title){ y_var \u0026lt;- sym(y_var) ggplot(data = df, aes(x = Scenario, y = !!y_var, group = Cluster_method, color = Cluster_method)) + geom_line(size = 1.25, alpha = .6) + geom_point(color = \u0026quot;black\u0026quot;, size = 2, alpha = .5) + ggrepel::geom_label_repel(data = filter(df, Scenario == \u0026quot;S4\u0026quot;), aes(x = Scenario, y = !!y_var, color = Cluster_method, label = Cluster_method), size = 4, hjust = 0, nudge_x = .5, direction = \u0026quot;y\u0026quot;, label.padding = .1, point.padding = .5, segment.size = .25, show.legend = F) + scale_x_discrete(limits = c(\u0026quot;S1\u0026quot;, \u0026quot;S2\u0026quot;, \u0026quot;S3\u0026quot;, \u0026quot;S4\u0026quot;, rep(\u0026quot;\u0026quot;, 4))) + scale_y_continuous(name = y_title, limits = c(0, NA)) + theme_minimal() + theme(legend.position = \u0026quot;none\u0026quot;) } cluster_plot \u0026lt;- create_plot(df = cluster_summary, y_var = \u0026quot;Clusters_vs_Groups\u0026quot;, y_title = \u0026quot;Detected # Clusters / True # Groups\u0026quot;) dyad_plot \u0026lt;- create_plot(df = cluster_summary, y_var = \u0026quot;Dyads_correct_prop\u0026quot;, y_title = \u0026quot;Correct # Dyads / Total # Dyads\u0026quot;) modularity_plot \u0026lt;- create_plot(df = cluster_summary, y_var = \u0026quot;Modularity\u0026quot;, y_title = \u0026quot;Modularity\u0026quot;) cowplot::plot_grid(cluster_plot, dyad_plot, modularity_plot, ncol = 3) 2.6 Conclusions For the created networks, here are my conclusions:\nNone(!) of the tested algorithms correctly identified the number of groups in S1 and S2 (the two scenarios with ‘close groups’). But three of the four algorithms that did best in these two scenarios (louvain, walktrap, optimal) perfectly identified groups in S3 and S4. Thus, under the circumstances simulated in S1 and S2, which are fairly common in reality I believe, one should be careful with the interpretation of the number of detected clusters. With the exception of S1, the infomap algorithm did not very well and put all individuals in the same cluster. The label_prob method was a bit better than infomap, but always failed to identify the correct number of groups. The comparison of modularity values confirms that the algorithms with the highest modularity values also were the best in identifying groups and individual group memberships (note that these modularity values are calculated without knowing the ‘true’ groups). As mentioned above and shown in the plot below, the simulation actually created many, fairly weak relationships among individuals, even among individuals from different groups. Perhaps, this makes the detection of clusters challenging for the algorithms, especially if the distinction between different groups within a larger community is variable (as in S1, S2). Thus, when applying such methods to identify clusters, it seems to be important to simulate some networks similar to the observed networks and try which methods are most appropriate given the circumstances. 2.7 Identifying and illustrating clusters with ggraph The ggraph package also supports the use of the igraph::cluster_ functions, which can be nicely used for a quick look at clusters. Here, both the detected clusters (using different colors) and the simulated groups (using different shapes) are shown. In contrast to the plots above, all relationships (or edges) are shown, including those \u0026lt; 0.3, which makes the networks look much denser than in the other plots.\nnetwork_df %\u0026gt;% select(from = Ind_A, to = Ind_B, weight = S1) %\u0026gt;% as_tbl_graph(directed = FALSE) %\u0026gt;% activate(nodes) %\u0026gt;% left_join(distinct(individual_df, name = Ind, group = Group), by = \u0026quot;name\u0026quot;) %\u0026gt;% mutate(detected_cluster = as.factor(group_louvain())) %\u0026gt;% ggraph(., layout = \u0026quot;fr\u0026quot;) + geom_edge_arc(aes(width = weight), alpha = 0.3, strength = 0.1) + scale_edge_width(name = \u0026quot;Association Index\u0026quot;, range = c(0, 1.5)) + geom_node_point(aes(fill = detected_cluster, shape = group), color = \u0026quot;black\u0026quot;, size = 4) + scale_fill_brewer(type = \u0026quot;qual\u0026quot;, palette = 2) + scale_shape_manual(values = c(21, 22, 23, 24)) + theme_graph() + guides(fill = guide_legend(override.aes = list(shape=21))) It’s clear from this plot that group_a and group_b are put in the same cluster by the louvain method.\n","date":1578182400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578254400,"objectID":"5809ad655c73d60698cad0797dd6737b","permalink":"https://bedatablog.netlify.com/post/social-clusters-ii/","publishdate":"2020-01-05T00:00:00Z","relpermalink":"/post/social-clusters-ii/","section":"post","summary":"In the previous post, I simulated clustered networks with varying strength of within-group, between-close-groups, and between-group social relationships. In part 2, I am using some of the methods provided by the igraph package to test how well these simulated clusters can be (re)-detected.\n2. Determine clustering using different methods and compare the results 2.1 Prepare R rm(list = ls()) library(tidyverse) library(tidygraph) library(ggraph) library(igraph) In addition to these packages, this notebook requires the installation of the following packages: DT, cowplot, ggrepel.","tags":["R","Social Networks","ggraph","igraph"],"title":"Creation and Detection of Clusters in Social Networks - Part 2","type":"post"},{"authors":[],"categories":[],"content":" Social networks often exhibit some kind of clustering (or community structure), such as distinct social groups in animal societies, or kin groups (or families) within social groups. Individuals within such clusters are more likely to interact with each other than individuals from different clusters.\nThere are many algorithms to detect clusters in social networks, and one might work better than another under some circumstances (see, e.g., Emmons et al. 2016). Therefore, the aim of this and the next post is to simulate clustered social networks, and then to apply and compare some of the methods provided by the R-package igraph with these networks.\nNote that there are functions available for R to create such networks, but here I create these networks ‘from scratch’ for two reasons:\nIt is easier to understand (and modify) the process underlying the network simulation. Some clusters might be ‘closer’ to each other than other clusters. For example, individuals from two close families might be more likely to interact with each other than with individuals from other families. Perhaps, this can be achieved with the available functions, but I preferred to model these different degrees of clustering on my own. This project will consist of two parts:\n1. Simulate clustered social networks. I will simulate values of an ‘association index’ (reflecting social relationships) for dyads of individuals within the same groups, for dyads of individuals from ‘close’ groups, and for dyads consisting of individuals from different groups that are not close to each other. In total, I will simulate four different scenarios with varying degrees of clustering.\n2. Determine clustering using different methods and compare the results. I will use several of the methods provided by the igraph package to detect clusters in these simulated networks. Then, I will calculate and compare the modularity of different networks, the number of detected clusters in comparison to simulated clusters, and the proportion of dyads correctly assigned to the same cluster.\n1. Simulate clustered social networks 1.1 Prepare R rm(list = ls()) library(tidyverse) library(tidygraph) library(ggraph) library(igraph) 1.2 Create a dataframe with identities and group memberships of individuals # Specify number of individuals and groups (or clusters) n_inds \u0026lt;- 50 n_groups \u0026lt;- 4 # Create IDs for all individuals and assign random social groups to individuals set.seed(1209) individual_df \u0026lt;- tibble( Ind = paste0(\u0026quot;ind_\u0026quot;, str_pad(1:n_inds, width = nchar(n_inds), pad = \u0026quot;0\u0026quot;)), Group = sample(x = paste0(\u0026quot;group_\u0026quot;, letters[1:n_groups]), size = n_inds, replace = T)) # Create a dataframe with all possible combinations of individuals except \u0026#39;self-relationships\u0026#39; network_df \u0026lt;- expand.grid(Ind_A = individual_df$Ind, Ind_B = individual_df$Ind, stringsAsFactors = F) %\u0026gt;% left_join(select(individual_df, Ind_A = Ind, Ind_A_Group = Group), by = \u0026quot;Ind_A\u0026quot;) %\u0026gt;% left_join(select(individual_df, Ind_B = Ind, Ind_B_Group = Group), by = \u0026quot;Ind_B\u0026quot;) %\u0026gt;% filter(Ind_A != Ind_B) # Limit the dataframe to one row per dyad for an undirected network network_df \u0026lt;- network_df %\u0026gt;% arrange(Ind_A, Ind_B) %\u0026gt;% mutate(dyad = if_else(Ind_A \u0026lt; Ind_B, paste0(Ind_A, \u0026quot;_\u0026quot;, Ind_B), paste0(Ind_B, \u0026quot;_\u0026quot;, Ind_A))) %\u0026gt;% distinct(dyad, .keep_all = T) head(network_df) ## Ind_A Ind_B Ind_A_Group Ind_B_Group dyad ## 1 ind_01 ind_02 group_b group_a ind_01_ind_02 ## 2 ind_01 ind_03 group_b group_c ind_01_ind_03 ## 3 ind_01 ind_04 group_b group_b ind_01_ind_04 ## 4 ind_01 ind_05 group_b group_d ind_01_ind_05 ## 5 ind_01 ind_06 group_b group_a ind_01_ind_06 ## 6 ind_01 ind_07 group_b group_b ind_01_ind_07 1.3 Assign values (or ‘weights’) to all dyadic relationships. Here, I use 4 different scenarios:\n1. Scenario (S1): Within-group relationships (wgr) are relatively strong in comparison to between-group relationships (bgr). Relationships of individuals between ‘close groups’ (bcgr) are intermediate.\n2. Scenario (S2): Similar to S1, but differences between wgr, bcgr, and bgr are smaller.\n3. Scenario (S3): Wgr are relatively strong in comparison to bgr. This scenario is identical to S1 except that there are no close groups.\n4. Scenario (S4): Differences between wgr and bgr are smaller than in S3. This scenario is identical to S2 except that there are no close groups.\nThus, bgr are the same in all four scenarios, only bcgr and wgr are varied. Furthermore, wgr are the same for S1 and S3, and for S2 and S4 (see plot below).\nTo create the social networks, I sample values from beta distributions simulating an Association Index. Such an index is commonly used in animal behavioural research to assess relationships between individuals (see, e.g., the great book by Whitehead, 2008), and they range from 0 to 1. For example, individuals that never associate with each other would have an association index of 0, and two individuals that are always associated with each other have an index of 1.\n# Set the parameters (i.e. shape parameters of the beta distribution) for all 4 # scenarios bgr \u0026lt;- c(1, 8) bcgr_1 \u0026lt;- c(3, 8) wgr_1 \u0026lt;- wgr_3 \u0026lt;- c(8, 8) bcgr_2 \u0026lt;- c(1.5, 8) wgr_2 \u0026lt;- wgr_4 \u0026lt;- c(4, 8) # Define function for plotting of beta distributions plot_beta \u0026lt;- function(x, alpha_beta, main = \u0026quot;\u0026quot;, xlab = \u0026quot;\u0026quot;, ylab = \u0026quot;\u0026quot;, ...){ alpha = alpha_beta[1] beta = alpha_beta[2] plot(x, dbeta(x, alpha, beta), type = \u0026quot;l\u0026quot;, yaxt = \u0026quot;n\u0026quot;, xlab = \u0026quot;\u0026quot;, ylab = \u0026quot;\u0026quot;, bty = \u0026quot;n\u0026quot;, ...) title(ylab = ylab, line = 0) title(xlab = xlab, line = 2) title(main = main, line = 1) axis(side = 2, label = F, lwd.ticks = F) } # Plot the distributions of social relationships according to these parameters { par(mfrow = c(4,3), mar = c(3, 2, 2, 1)) x \u0026lt;- seq(0, 1, length.out = 40) # S1 plot_beta(x, bgr, main = \u0026quot;Between-group - S1\u0026quot;, ylab = \u0026quot;Prob. density function\u0026quot;) plot_beta(x, bcgr_1, main = \u0026quot;Between close-groups - S1\u0026quot;) plot_beta(x, wgr_1, main = \u0026quot;Within-group S1\u0026quot;) # S2 plot_beta(x, bgr, main = \u0026quot;Between-group - S2\u0026quot;, ylab = \u0026quot;Prob. density function\u0026quot;) plot_beta(x, bcgr_2, main = \u0026quot;Between close-groups - S2\u0026quot;, xlab = \u0026quot;Association Index\u0026quot;) plot_beta(x, wgr_2, main = \u0026quot;Within-group - S2\u0026quot;) # S3 plot_beta(x, bgr, main = \u0026quot;Between-group - S3\u0026quot;, ylab = \u0026quot;Prob. density function\u0026quot;) plot.new() plot_beta(x, wgr_3, main = \u0026quot;Within-group - S3\u0026quot;) # S4 plot_beta(x, bgr, main = \u0026quot;Between-group - S4\u0026quot;, ylab = \u0026quot;Prob. density function\u0026quot;, xlab = \u0026quot;Association Index\u0026quot;) plot.new() plot_beta(x, wgr_4, main = \u0026quot;Within-group - S4\u0026quot;, xlab = \u0026quot;Association Index\u0026quot;) } These distributions can be used to get values for the strength of relationships. To do so, I first define a function to sample dyadic values from the different distributions depending on group membership of both individuals. Then, I use this function to get values for all dyads and for all scenarios. For S1 and S2, group A and B are defined as the close groups.\nget_weights \u0026lt;- function(network_df, wgr, bgr, bcgr = NA, close_groups = NA){ # Create empty vector for all weights weights = rep(NA_real_, time = nrow(network_df)) # Go through all dyads and sample from respective distribution, dependent on # the group membership of both individuals for(i in seq_along(weights)){ ind_A_group \u0026lt;- network_df[i, \u0026quot;Ind_A_Group\u0026quot;] ind_B_group \u0026lt;- network_df[i, \u0026quot;Ind_B_Group\u0026quot;] # Both individuals in same group if(ind_A_group == ind_B_group){ weights[i] \u0026lt;- rbeta(n = 1, shape1 = wgr[1], shape2 = wgr[2]) } # Individuals in different groups if(ind_A_group != ind_B_group){ weights[i] \u0026lt;- rbeta(n = 1, shape1 = bgr[1], shape2 = bgr[2]) } # If some groups are \u0026#39;closer\u0026#39; to each other, use specified distributions for # this kind of relationships if(all(!is.na(bcgr)) \u0026amp; all(!is.na(close_groups)) \u0026amp; ind_A_group != ind_B_group \u0026amp; ind_A_group %in% close_groups \u0026amp; ind_B_group %in% close_groups){ weights[i] \u0026lt;- rbeta(n = 1, shape1 = bcgr[1], shape2 = bcgr[2]) } } return(weights) } set.seed(1209) network_df$S1 \u0026lt;- get_weights(network_df, wgr = wgr_1, bgr = bgr, bcgr = bcgr_1, close_groups = c(\u0026quot;group_a\u0026quot;, \u0026quot;group_b\u0026quot;)) network_df$S2 \u0026lt;- get_weights(network_df, wgr = wgr_2, bgr = bgr, bcgr = bcgr_2, close_groups = c(\u0026quot;group_a\u0026quot;, \u0026quot;group_b\u0026quot;)) network_df$S3 \u0026lt;- get_weights(network_df, wgr = wgr_3, bgr = bgr) network_df$S4 \u0026lt;- get_weights(network_df, wgr = wgr_4, bgr = bgr) As the last step for part 1 of this post, I will illustrate the created networks using the ggraph package (look here for an introduction to this package by the author)\nset.seed(1209) clustered_network_plot \u0026lt;- network_df %\u0026gt;% pivot_longer(cols = matches(\u0026quot;S\\\\d\u0026quot;), names_to = \u0026quot;Scenario\u0026quot;, values_to = \u0026quot;weight\u0026quot;) %\u0026gt;% select(from = Ind_A, to = Ind_B, weight, Scenario) %\u0026gt;% filter(weight \u0026gt;= 0.3) %\u0026gt;% as_tbl_graph() %\u0026gt;% activate(nodes) %\u0026gt;% left_join(distinct(individual_df, name = Ind, group = Group), by = \u0026quot;name\u0026quot;) %\u0026gt;% ggraph(., layout = \u0026quot;fr\u0026quot;) + geom_edge_arc(aes(width = weight), alpha = 0.4, strength = 0.1) + scale_edge_width(name = \u0026quot;Association Index\u0026quot;, range = c(0.2, 1)) + geom_node_point(aes(fill = group, shape = group), size = 2) + scale_fill_brewer(type = \u0026quot;qual\u0026quot;, palette = 2) + scale_shape_manual(values = c(21, 22, 23, 24)) + facet_edges(~Scenario) + theme_graph() clustered_network_plot As modeled above, S1 and S3 are very similar, except that group A and B are closer to each other in S1. S2 and S4 have both weaker within-group relationships compared to their counterparts S1 and S3, respectively. Thus, I have four different networks with different degrees of clustering, and in two of these networks, two of the four groups are closer to each other than to the other groups.\nIn the next post, I will use different algorithms from the igraph package to check how well these clusters can be detected.\n","date":1577750400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578053796,"objectID":"b0f93e98d4f3cce78629c6e9e83567f3","permalink":"https://bedatablog.netlify.com/post/social-clusters-i/","publishdate":"2019-12-31T00:00:00Z","relpermalink":"/post/social-clusters-i/","section":"post","summary":"Social networks often exhibit some kind of clustering (or community structure), such as distinct social groups in animal societies, or kin groups (or families) within social groups. Individuals within such clusters are more likely to interact with each other than individuals from different clusters.\nThere are many algorithms to detect clusters in social networks, and one might work better than another under some circumstances (see, e.g., Emmons et al. 2016).","tags":["R","Social Networks","ggraph","igraph"],"title":"Creation and Detection of Clusters in Social Networks - Part 1","type":"post"}]